# -*- coding: utf-8 -*-
"""train_recommender.py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sVKOtGbw7O2A68wNS6EH3nfvn5FeeraV
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import joblib

print("Starting AI Recommendation Model Training...")

# Step 1: Create our training dataset (Week 5: Data Collection)
# In a real project, this would come from experts.
data = {
    'degradation_percent': [
        5, 15, 25, 35, 45, 55, 65, 75, 85,  # Agricultural
        5, 15, 25, 35, 45, 55, 65, 75, 85,  # Forest
        5, 15, 25, 35, 45, 55, 65, 75, 85   # Other
    ],
    'land_use_type': [ # 0=Agricultural, 1=Forest, 2=Other
        0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1,
        2, 2, 2, 2, 2, 2, 2, 2, 2
    ],
    'recommendation_class': [ # 0=Maintain, 1=Monitor, 2=Moderate, 3=Urgent
        # Agricultural Logic:
        0, # 5%
        1, # 15%
        2, # 25%
        2, # 35%
        3, # 45%
        3, # 55%
        3, # 65%
        3, # 75%
        3, # 85%
        # Forest Logic:
        0, # 5%
        0, # 15%
        1, # 25%
        1, # 35%
        2, # 45%
        2, # 55%
        3, # 65%
        3, # 75%
        3, # 85%
        # Other Logic:
        0, # 5%
        1, # 15%
        1, # 25%
        2, # 35%
        2, # 45%
        2, # 55%
        2, # 65%
        3, # 75%
        3  # 85%
    ]
}

# Create a more robust dataset by adding noise
for _ in range(500):
    base_deg = np.random.randint(0, 90)
    base_land = np.random.randint(0, 3)

    # Add noise
    noise_deg = base_deg + np.random.randint(-5, 5)

    # Simple logic for target
    if noise_deg < 10: target = 0
    elif noise_deg < 25: target = 1
    elif noise_deg < 40: target = 2
    else: target = 3

    # Make forest more resilient
    if base_land == 1 and noise_deg < 30: target = 1

    data['degradation_percent'].append(np.clip(noise_deg, 0, 100))
    data['land_use_type'].append(base_land)
    data['recommendation_class'].append(target)

df = pd.DataFrame(data)
print(f"Created dataset with {len(df)} samples.")

# Step 2: Define Features (X) and Target (y) (Week 5: Preprocessing)
X = df[['degradation_percent', 'land_use_type']]
y = df['recommendation_class']

# Step 3: Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Training on {len(X_train)} samples, testing on {len(X_test)} samples.")

# Step 4: Train the AI Model (Week 2: ML Paradigms)
# We use a RandomForest, a powerful and common classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
print("Model training complete.")

# Step 5: Evaluate the Model
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy * 100:.2f}%")

# Step 6: Save the Trained Model (Week 5: Deployment/Integration)
model_filename = "recommendation_model.joblib"
joblib.dump(model, model_filename)
print(f"Successfully trained and saved model to '{model_filename}'")